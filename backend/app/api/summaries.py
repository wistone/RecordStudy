from fastapi import APIRouter, Depends, HTTPException, Query
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta, date, timezone
import pytz
from app.core.auth import get_current_user_id
from supabase import create_client
from app.core.config import settings
import json
import sys
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ç”¨æˆ·æ—¶åŒºè®¾ç½®ï¼ˆé»˜è®¤ä¸ºä¸­å›½æ—¶é—´ï¼Œåç»­å¯ä»¥ä»ç”¨æˆ·é…ç½®ä¸­è·å–ï¼‰
USER_TIMEZONE = pytz.timezone('Asia/Shanghai')

def utc_to_local_date(utc_datetime):
    """å°†UTCæ—¶é—´è½¬æ¢ä¸ºæœ¬åœ°æ—¶åŒºçš„æ—¥æœŸ"""
    if utc_datetime.tzinfo is None:
        utc_datetime = utc_datetime.replace(tzinfo=timezone.utc)
    local_datetime = utc_datetime.astimezone(USER_TIMEZONE)
    return local_datetime.date()

def get_local_date_boundaries(days_back=0):
    """è·å–æœ¬åœ°æ—¶åŒºçš„æ—¥æœŸè¾¹ç•Œï¼ˆå¼€å§‹æ—¶é—´å’Œç»“æŸæ—¶é—´çš„UTCè¡¨ç¤ºï¼‰"""
    # è·å–æœ¬åœ°æ—¶é—´çš„ä»Šå¤©
    local_now = datetime.now(USER_TIMEZONE)
    local_today = local_now.date()
    
    # è®¡ç®—ç›®æ ‡æ—¥æœŸ
    target_date = local_today - timedelta(days=days_back)
    
    # æ„å»ºæœ¬åœ°æ—¶é—´çš„å¼€å§‹å’Œç»“æŸæ—¶é—´
    local_start = USER_TIMEZONE.localize(datetime.combine(target_date, datetime.min.time()))
    local_end = USER_TIMEZONE.localize(datetime.combine(local_today + timedelta(days=1), datetime.min.time()))
    
    # è½¬æ¢ä¸ºUTCæ—¶é—´ç”¨äºæ•°æ®åº“æŸ¥è¯¢
    utc_start = local_start.astimezone(timezone.utc)
    utc_end = local_end.astimezone(timezone.utc)
    
    return utc_start, utc_end

def safe_parse_datetime(datetime_str):
    """å®‰å…¨è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œè¿”å›UTCæ—¶é—´"""
    try:
        occurred_at = datetime_str
        # å¤„ç†ä¸åŒçš„æ—¥æœŸæ ¼å¼
        if occurred_at.endswith('Z'):
            occurred_at = occurred_at.replace('Z', '+00:00')
        elif '+' not in occurred_at and 'T' in occurred_at:
            occurred_at = occurred_at + '+00:00'
        
        # å¤„ç†å¾®ç§’ç²¾åº¦é—®é¢˜
        if '.' in occurred_at:
            parts = occurred_at.split('.')
            if len(parts) == 2:
                microseconds = parts[1].split('+')[0].split('-')[0]
                # ç¡®ä¿å¾®ç§’éƒ¨åˆ†æ˜¯6ä½æ•°
                if len(microseconds) > 6:
                    microseconds = microseconds[:6]
                elif len(microseconds) < 6:
                    microseconds = microseconds.ljust(6, '0')
                
                timezone_part = parts[1][len(microseconds.rstrip('0') or '0'):]
                occurred_at = f"{parts[0]}.{microseconds}{timezone_part}"
        
        return datetime.fromisoformat(occurred_at)
    except (ValueError, AttributeError) as e:
        print(f"âš ï¸ æ—¥æœŸè§£æå¤±è´¥: {datetime_str} - {e}")
        return None

router = APIRouter()

# ç®€å•çš„å†…å­˜ç¼“å­˜ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨Redisï¼‰
_cache = {}
_cache_expiry = {}
CACHE_DURATION = 300  # 5åˆ†é’Ÿç¼“å­˜

def get_cache_key(user_id: str, cache_type: str, params: str = "") -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    return f"{user_id}:{cache_type}:{params}"

def get_from_cache(key: str) -> Optional[Any]:
    """ä»ç¼“å­˜è·å–æ•°æ®"""
    if key in _cache and key in _cache_expiry:
        if datetime.now() < _cache_expiry[key]:
            return _cache[key]
        else:
            # ç¼“å­˜è¿‡æœŸï¼Œæ¸…ç†
            del _cache[key]
            del _cache_expiry[key]
    return None

def set_cache(key: str, data: Any, duration: int = CACHE_DURATION):
    """è®¾ç½®ç¼“å­˜"""
    _cache[key] = data
    _cache_expiry[key] = datetime.now() + timedelta(seconds=duration)

@router.get("/dashboard", response_model=Dict[str, Any])
async def get_dashboard_summary(
    days: int = Query(7, ge=1, le=30, description="ç»Ÿè®¡æœ€è¿‘Nå¤©"),
    current_user_id: str = Depends(get_current_user_id)
):
    """è·å–é¦–é¡µä»ªè¡¨ç›˜æ±‡æ€»æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    
    # æ£€æŸ¥ç¼“å­˜
    cache_key = get_cache_key(current_user_id, "dashboard", str(days))
    cached_data = get_from_cache(cache_key)
    if cached_data:
        return cached_data
    
    try:
        client = create_client(settings.SUPABASE_URL, settings.SUPABASE_SERVICE_KEY)
        
        # ä½¿ç”¨æœ¬åœ°æ—¶åŒºè®¡ç®—æ—¥æœŸè¾¹ç•Œ
        utc_start, utc_end = get_local_date_boundaries(days - 1)
        
        # è·å–è®°å½•æ•°æ®ï¼ˆåªæŸ¥è¯¢å¿…è¦å­—æ®µï¼‰
        response = client.table('records')\
            .select('occurred_at, duration_min, form_type, difficulty, focus')\
            .eq('user_id', current_user_id)\
            .gte('occurred_at', utc_start.isoformat())\
            .lt('occurred_at', utc_end.isoformat())\
            .execute()
        
        records = response.data or []
        
        # è®¡ç®—æ±‡æ€»æ•°æ®
        total_records = len(records)
        total_duration = sum(r.get('duration_min', 0) or 0 for r in records)
        
        # è®¡ç®—å­¦ä¹ å¤©æ•°ï¼ˆæœ‰è®°å½•çš„å¤©æ•°ï¼‰- ä½¿ç”¨æœ¬åœ°æ—¶åŒº
        learning_dates = set()
        for record in records:
            if record.get('occurred_at'):
                utc_datetime = safe_parse_datetime(record['occurred_at'])
                if utc_datetime:
                    # è½¬æ¢ä¸ºæœ¬åœ°æ—¶åŒºçš„æ—¥æœŸ
                    local_date = utc_to_local_date(utc_datetime)
                    learning_dates.add(local_date)
        
        learning_days = len(learning_dates)
        
        # è®¡ç®—å¹³å‡åˆ†æ•°ï¼ˆè·³è¿‡Noneå€¼ï¼‰
        valid_difficulty = [r['difficulty'] for r in records if r.get('difficulty') is not None]
        valid_focus = [r['focus'] for r in records if r.get('focus') is not None]
        
        avg_difficulty = sum(valid_difficulty) / len(valid_difficulty) if valid_difficulty else 0
        avg_focus = sum(valid_focus) / len(valid_focus) if valid_focus else 0
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        type_stats = {}
        for record in records:
            form_type = record.get('form_type', 'other')
            if form_type not in type_stats:
                type_stats[form_type] = {'count': 0, 'duration': 0}
            type_stats[form_type]['count'] += 1
            type_stats[form_type]['duration'] += record.get('duration_min', 0) or 0
        
        type_distribution = [
            {
                "type": k,
                "count": v['count'],
                "total_duration": v['duration']
            }
            for k, v in type_stats.items()
        ]
        
        # æ³¨æ„: consecutive_days åœ¨ init endpoint ä¸­å…¨å±€è®¡ç®—ï¼Œè¿™é‡Œä¸éœ€è¦è®¡ç®—
        
        # ä»Šæ—¥ç»Ÿè®¡ï¼ˆä½¿ç”¨æœ¬åœ°æ—¶åŒºï¼‰
        today_records = []
        for r in records:
            if r.get('occurred_at'):
                utc_datetime = safe_parse_datetime(r['occurred_at'])
                if utc_datetime:
                    local_date = utc_to_local_date(utc_datetime)
                    if local_date == today:
                        today_records.append(r)
        
        today_count = len(today_records)
        today_duration = sum(r.get('duration_min', 0) or 0 for r in today_records)
        
        # ç»„è£…ç»“æœ
        dashboard_data = {
            "period_days": days,
            "total_records": total_records,
            "total_duration_hours": round(total_duration / 60, 1) if total_duration else 0,
            "learning_days": learning_days,
            "streak_days": consecutive_days,
            "avg_difficulty": round(avg_difficulty, 1),
            "avg_focus": round(avg_focus, 1),
            "daily_avg_duration": round(total_duration / max(learning_days, 1), 1) if total_duration else 0,
            "type_distribution": type_distribution,
            "today": {
                "count": today_count,
                "duration_minutes": today_duration
            },
            "cache_timestamp": datetime.now().isoformat()
        }
        
        # ç¼“å­˜ç»“æœ
        set_cache(cache_key, dashboard_data)
        
        return dashboard_data
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to fetch dashboard summary: {str(e)}"
        )

@router.get("/recent-records", response_model=Dict[str, Any])
async def get_recent_records_summary(
    limit: int = Query(10, ge=1, le=50, description="æœ€è¿‘è®°å½•æ•°é‡"),
    current_user_id: str = Depends(get_current_user_id)
):
    """è·å–æœ€è¿‘è®°å½•çš„ç®€åŒ–ä¿¡æ¯ï¼ˆç”¨äºé¦–é¡µæ˜¾ç¤ºï¼‰"""
    
    cache_key = get_cache_key(current_user_id, "recent", str(limit))
    cached_data = get_from_cache(cache_key)
    if cached_data:
        return cached_data
    
    try:
        client = create_client(settings.SUPABASE_URL, settings.SUPABASE_SERVICE_KEY)
        
        # åªæŸ¥è¯¢é¦–é¡µéœ€è¦çš„å­—æ®µ
        response = client.table('records')\
            .select('record_id, title, form_type, occurred_at, duration_min')\
            .eq('user_id', current_user_id)\
            .order('occurred_at', desc=True)\
            .limit(limit)\
            .execute()
        
        records = []
        for record in response.data or []:
            # ç®€åŒ–çš„è®°å½•ä¿¡æ¯
            records.append({
                "id": record['record_id'],
                "title": record['title'],
                "type": record['form_type'],
                "occurred_at": record['occurred_at'],
                "duration_min": record.get('duration_min', 0)
            })
        
        recent_data = {
            "records": records,
            "total": len(records),
            "cache_timestamp": datetime.now().isoformat()
        }
        
        # ç¼“å­˜ç»“æœ
        set_cache(cache_key, recent_data, 180)  # 3åˆ†é’Ÿç¼“å­˜
        
        return recent_data
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to fetch recent records: {str(e)}"
        )

@router.post("/invalidate-cache")
async def invalidate_user_cache(current_user_id: str = Depends(get_current_user_id)):
    """æ‰‹åŠ¨æ¸…é™¤ç”¨æˆ·ç¼“å­˜ï¼ˆå½“åˆ›å»ºæ–°è®°å½•æ—¶è°ƒç”¨ï¼‰"""
    try:
        # æ¸…é™¤è¯¥ç”¨æˆ·çš„æ‰€æœ‰ç¼“å­˜
        keys_to_remove = []
        for key in _cache.keys():
            if key.startswith(f"{current_user_id}:"):
                keys_to_remove.append(key)
        
        for key in keys_to_remove:
            if key in _cache:
                del _cache[key]
            if key in _cache_expiry:
                del _cache_expiry[key]
        
        return {"message": f"Invalidated {len(keys_to_remove)} cache entries", "user_id": current_user_id}
        
    except Exception as e:
        return {"error": str(e), "message": "Failed to invalidate cache"}

@router.get("/cache-stats", response_model=Dict[str, Any])
async def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰"""
    now = datetime.now()
    valid_entries = sum(1 for key, expiry in _cache_expiry.items() if expiry > now)
    expired_entries = len(_cache) - valid_entries
    
    return {
        "total_entries": len(_cache),
        "valid_entries": valid_entries,
        "expired_entries": expired_entries,
        "cache_duration_seconds": CACHE_DURATION
    }

@router.get("/init", response_model=Dict[str, Any])
async def get_init_data(
    current_user_id: str = Depends(get_current_user_id)
):
    """èšåˆåˆå§‹åŒ–API - ä¸€æ¬¡è°ƒç”¨è·å–æ‰€æœ‰é¦–é¡µæ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    
    # æ£€æŸ¥ç¼“å­˜
    cache_key = get_cache_key(current_user_id, "init_data", "")
    cached_data = get_from_cache(cache_key)
    if cached_data:
        return cached_data
    
    try:
        client = create_client(settings.SUPABASE_URL, settings.SUPABASE_SERVICE_KEY)
        
        # å¹¶è¡ŒæŸ¥è¯¢æ‰€æœ‰å¿…è¦æ•°æ®
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        def get_dashboard_data(days):
            """è·å–æŒ‡å®šå¤©æ•°çš„ä»ªè¡¨ç›˜æ•°æ®"""
            utc_start, utc_end = get_local_date_boundaries(days - 1)
            
            response = client.table('records')\
                .select('occurred_at, duration_min, form_type, difficulty, focus')\
                .eq('user_id', current_user_id)\
                .gte('occurred_at', utc_start.isoformat())\
                .lt('occurred_at', utc_end.isoformat())\
                .execute()
            
            records = response.data or []
            total_records = len(records)
            total_duration = sum(r.get('duration_min', 0) or 0 for r in records)
            
            # è®¡ç®—å­¦ä¹ å¤©æ•°
            learning_dates = set()
            for record in records:
                if record.get('occurred_at'):
                    utc_datetime = safe_parse_datetime(record['occurred_at'])
                    if utc_datetime:
                        local_date = utc_to_local_date(utc_datetime)
                        learning_dates.add(local_date)
            
            learning_days = len(learning_dates)
            
            # è®¡ç®—å¹³å‡åˆ†æ•°
            valid_difficulty = [r['difficulty'] for r in records if r.get('difficulty') is not None]
            valid_focus = [r['focus'] for r in records if r.get('focus') is not None]
            avg_difficulty = sum(valid_difficulty) / len(valid_difficulty) if valid_difficulty else 0
            avg_focus = sum(valid_focus) / len(valid_focus) if valid_focus else 0
            
            # æŒ‰ç±»å‹ç»Ÿè®¡
            type_stats = {}
            for record in records:
                form_type = record.get('form_type', 'other')
                if form_type not in type_stats:
                    type_stats[form_type] = {'count': 0, 'duration': 0}
                type_stats[form_type]['count'] += 1
                type_stats[form_type]['duration'] += record.get('duration_min', 0) or 0
            
            type_distribution = [
                {"type": k, "count": v['count'], "total_duration": v['duration']}
                for k, v in type_stats.items()
            ]
            
            # æ³¨æ„: consecutive_days åœ¨ init endpoint ä¸­å…¨å±€è®¡ç®—ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤è®¡ç®—
            
            return {
                "period_days": days,
                "total_records": total_records,
                "total_duration_hours": round(total_duration / 60, 1) if total_duration else 0,
                "learning_days": learning_days,
                "avg_difficulty": round(avg_difficulty, 1),
                "avg_focus": round(avg_focus, 1),
                "daily_avg_duration": round(total_duration / max(learning_days, 1), 1) if total_duration else 0,
                "type_distribution": type_distribution,
                "learning_dates": [d.isoformat() for d in learning_dates]  # åºåˆ—åŒ–dateå¯¹è±¡
            }
        
        def get_recent_records():
            """è·å–æœ€è¿‘è®°å½•"""
            # å…ˆè·å–åŸºç¡€è®°å½•ä¿¡æ¯
            response = client.table('records')\
                .select('record_id, title, form_type, occurred_at, duration_min, resource_id')\
                .eq('user_id', current_user_id)\
                .order('occurred_at', desc=True)\
                .limit(20)\
                .execute()
            
            records = []
            resource_ids = []
            
            # æ”¶é›†æœ‰resource_idçš„è®°å½•
            for record in response.data or []:
                records.append({
                    "record_id": record['record_id'],  # ä½¿ç”¨record_idå­—æ®µä¿æŒä¸€è‡´
                    "title": record['title'],
                    "form_type": record['form_type'],  # ä½¿ç”¨form_typeå­—æ®µä¿æŒä¸€è‡´
                    "occurred_at": record['occurred_at'],
                    "duration_min": record.get('duration_min', 0),
                    "resource_id": record.get('resource_id'),
                    "tags": []
                })
                if record.get('resource_id'):
                    resource_ids.append(record['resource_id'])
            
            # å¦‚æœæœ‰resource_idsï¼Œæ‰¹é‡è·å–æ ‡ç­¾
            if resource_ids:
                try:
                    # è·å–èµ„æºæ ‡ç­¾å…³è”
                    resource_tags_response = client.table('resource_tags')\
                        .select('resource_id, tag_id')\
                        .eq('user_id', current_user_id)\
                        .in_('resource_id', resource_ids)\
                        .execute()
                    
                    if resource_tags_response.data:
                        # æ”¶é›†æ ‡ç­¾ID
                        tag_ids = list(set([rt['tag_id'] for rt in resource_tags_response.data]))
                        
                        if tag_ids:
                            # è·å–æ ‡ç­¾è¯¦æƒ…
                            tags_response = client.table('tags')\
                                .select('tag_id, tag_name')\
                                .in_('tag_id', tag_ids)\
                                .execute()
                            
                            # åˆ›å»ºtag_idåˆ°nameçš„æ˜ å°„
                            tag_map = {tag['tag_id']: tag['tag_name'] for tag in tags_response.data or []}
                            
                            # åˆ›å»ºresource_idåˆ°tagsçš„æ˜ å°„
                            resource_tag_map = {}
                            for rt in resource_tags_response.data:
                                resource_id = rt['resource_id']
                                tag_name = tag_map.get(rt['tag_id'])
                                if tag_name:
                                    if resource_id not in resource_tag_map:
                                        resource_tag_map[resource_id] = []
                                    resource_tag_map[resource_id].append({
                                        'name': tag_name,
                                        'color': '#gray'  # é»˜è®¤é¢œè‰²ï¼Œå¯ä»¥åç»­ä¼˜åŒ–
                                    })
                            
                            # ä¸ºè®°å½•æ·»åŠ æ ‡ç­¾
                            for record in records:
                                if record['resource_id'] and record['resource_id'] in resource_tag_map:
                                    record['tags'] = resource_tag_map[record['resource_id']]
                                # æ¸…é™¤resource_idå­—æ®µ
                                del record['resource_id']
                
                except Exception as e:
                    print(f"è·å–æ ‡ç­¾å¤±è´¥: {e}")
                    # ç»§ç»­å¤„ç†ï¼Œåªæ˜¯æ²¡æœ‰æ ‡ç­¾ä¿¡æ¯
            
            # æ¸…é™¤æ‰€æœ‰è®°å½•çš„resource_idå­—æ®µ
            for record in records:
                record.pop('resource_id', None)
            
            return records
        
        def get_form_types():
            """è·å–å­¦ä¹ å½¢å¼ç±»å‹"""
            try:
                # ä½¿ç”¨æ­£ç¡®çš„è¡¨åï¼šuser_form_types
                response = client.table('user_form_types')\
                    .select('*')\
                    .eq('user_id', current_user_id)\
                    .order('display_order', desc=False)\
                    .order('type_id', desc=False)\
                    .execute()
                
                return response.data or []
                
            except Exception as e:
                print(f"è·å–è¡¨å•ç±»å‹å¤±è´¥: {e}")
                # è¿”å›ç©ºåˆ—è¡¨ï¼Œè®©å‰ç«¯ä½¿ç”¨é»˜è®¤å€¼
                return []

        def get_user_profile():
            """è·å–ç”¨æˆ·èµ„æ–™"""
            try:
                response = client.table('profiles')\
                    .select('*')\
                    .eq('user_id', current_user_id)\
                    .limit(1)\
                    .execute()
                
                if response.data:
                    return response.data[0]
                else:
                    # å¦‚æœæ²¡æœ‰profileè®°å½•ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
                    return {
                        'user_id': current_user_id,
                        'display_name': None,
                        'avatar_url': None
                    }
                
            except Exception as e:
                print(f"è·å–ç”¨æˆ·èµ„æ–™å¤±è´¥: {e}")
                return {
                    'user_id': current_user_id,
                    'display_name': None,
                    'avatar_url': None
                }
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡ŒæŸ¥è¯¢
        with ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤æ‰€æœ‰æŸ¥è¯¢ä»»åŠ¡
            week_future = executor.submit(get_dashboard_data, 7)
            month_future = executor.submit(get_dashboard_data, 30)
            records_future = executor.submit(get_recent_records)
            form_types_future = executor.submit(get_form_types)
            profile_future = executor.submit(get_user_profile)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            week_summary = week_future.result()
            month_summary = month_future.result()
            recent_records = records_future.result()
            form_types = form_types_future.result()
            user_profile = profile_future.result()
        
        # è®¡ç®—è¿ç»­å­¦ä¹ å¤©æ•°
        learning_dates_str = week_summary.get('learning_dates', [])
        # å°†ISOå­—ç¬¦ä¸²è½¬æ¢å›dateå¯¹è±¡
        learning_dates = set()
        for date_str in learning_dates_str:
            try:
                learning_dates.add(datetime.fromisoformat(date_str).date())
            except (ValueError, AttributeError):
                continue
        
        consecutive_days = 0
        local_now = datetime.now(USER_TIMEZONE)
        today = local_now.date()
        yesterday = today - timedelta(days=1)
        
        # è°ƒè¯•æ—¥å¿—
        print(f"ğŸ” è¿ç»­å¤©æ•°è®¡ç®—è°ƒè¯•:")
        print(f"   ä»Šå¤©: {today}")
        print(f"   æ˜¨å¤©: {yesterday}")
        print(f"   å­¦ä¹ æ—¥æœŸ: {sorted(learning_dates)}")
        print(f"   ä»Šå¤©åœ¨å­¦ä¹ æ—¥æœŸä¸­: {today in learning_dates}")
        print(f"   æ˜¨å¤©åœ¨å­¦ä¹ æ—¥æœŸä¸­: {yesterday in learning_dates}")
        
        if today in learning_dates:
            check_date = today
            while check_date in learning_dates:
                consecutive_days += 1
                check_date = check_date - timedelta(days=1)
                print(f"   æ£€æŸ¥æ—¥æœŸ {check_date}: è¿ç»­å¤©æ•° = {consecutive_days}")
        elif yesterday in learning_dates:
            check_date = yesterday
            while check_date in learning_dates:
                consecutive_days += 1
                check_date = check_date - timedelta(days=1)
                print(f"   æ£€æŸ¥æ—¥æœŸ {check_date}: è¿ç»­å¤©æ•° = {consecutive_days}")
        
        print(f"   æœ€ç»ˆè¿ç»­å¤©æ•°: {consecutive_days}")
        
        # è®¡ç®—ä»Šæ—¥ç»Ÿè®¡
        today_records = [r for r in recent_records if r.get('occurred_at') and utc_to_local_date(safe_parse_datetime(r['occurred_at'])) == today]
        today_count = len(today_records)
        today_duration = sum(r.get('duration_min', 0) for r in today_records)
        
        # ç»„è£…æœ€ç»ˆå“åº”
        init_data = {
            "dashboard": {
                "week": {**week_summary, "streak_days": consecutive_days, "today": {"count": today_count, "duration_minutes": today_duration}},
                "month": {**month_summary, "streak_days": consecutive_days}
            },
            "recent_records": {
                "records": recent_records,
                "total": len(recent_records)
            },
            "form_types": form_types,
            "user_profile": user_profile,
            "cache_timestamp": datetime.now().isoformat()
        }
        
        # ç¼“å­˜ç»“æœï¼ˆ3åˆ†é’Ÿï¼‰
        set_cache(cache_key, init_data, 180)
        
        return init_data
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to fetch initialization data: {str(e)}"
        )